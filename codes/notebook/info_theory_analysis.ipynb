{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.special import psi\n",
    "import numpy.linalg as la\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.linalg import expm\n",
    "from scipy.special import digamma\n",
    "from sklearn.neighbors import BallTree, KDTree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# uint i to binary bin(i,q_bit): max(i)=15., q_bit=4\n",
    "# 该转换规则为一种样例，可考虑其他转二值序列方法。得到的序列用于输入下面的HKS_calculation函数中生成KS熵矩阵\n",
    "def int_to_binary(numbers):\n",
    "    binlist = np.array([f'{int(num):04b}' for num in numbers])\n",
    "    concatenated_string = ''.join(binlist)\n",
    "    sequence = np.array(list(concatenated_string), dtype=int)\n",
    "    return sequence\n",
    "\n",
    "# uint i to binary bin(i), random spikes (1) of total number i in 15 time windows\n",
    "# 另一种转换规则，即将int15映射到具有对应脉冲次数的15时间窗内\n",
    "def int_to_binary_random(numbers):\n",
    "    binary_arrays = [(np.arange(15) < val).astype(int) for val in numbers]\n",
    "    for binary_array in binary_arrays:\n",
    "        np.random.shuffle(binary_array)\n",
    "    sequence = np.hstack(binary_arrays)\n",
    "    return sequence\n",
    "\n",
    "# smooth the spike array\n",
    "# 对脉冲数组进行光滑处理\n",
    "def smooth_sequence(arr):\n",
    "    n = len(arr)\n",
    "    smoothed_arr = arr.copy()\n",
    "    \n",
    "    start_indices = np.concatenate(([0], np.where(np.diff(arr) != 0)[0] + 1))\n",
    "    end_indices = np.concatenate((start_indices[1:], [n]))\n",
    "    \n",
    "    for start, end in zip(start_indices, end_indices):\n",
    "        if end - start > 2:\n",
    "            x = np.arange(start, end)\n",
    "            y = arr[start:end]\n",
    "            try:\n",
    "                f = interp1d(x, y, kind='cubic')\n",
    "                smoothed_arr[start:end] = f(x)\n",
    "            except ValueError:\n",
    "                f = interp1d(x, y, kind='linear')\n",
    "                smoothed_arr[start:end] = f(x)\n",
    "        elif end - start == 2:\n",
    "            x = np.arange(start, end)\n",
    "            y = arr[start:end]\n",
    "            f = interp1d(x, y, kind='linear')\n",
    "            smoothed_arr[start:end] = f(x)\n",
    "        else:\n",
    "            smoothed_arr[start:end] = arr[start:end]\n",
    "    \n",
    "    smoothed_arr = np.array(smoothed_arr)\n",
    "    \n",
    "    return smoothed_arr\n",
    "\n",
    "# Input: a spike train (0/1); Output: the KS Entropy matrix of the spike train\n",
    "# 计算脉冲序列的KS熵值\n",
    "def HKS_calculation(spike_train, smooth = False):\n",
    "    # MLE of lambda in Poisson distribution equals to the sample average\n",
    "    spike_numlist = np.cumsum(spike_train)\n",
    "    lambda_mle_list = spike_numlist\n",
    "    if smooth == True:\n",
    "        lambda_mle_list = smooth_sequence(lambda_mle_list)\n",
    "\n",
    "    t_max = len(spike_train)\n",
    "    r_max = int(2*np.max(lambda_mle_list))\n",
    "    tau_max = int(t_max/5) # 默认计算时间长度t=0.8*t_max，前向时间长度tau=0.2*t_max，可手动调节\n",
    "    time_windows = 0.01\n",
    "\n",
    "    HKS_matrix = np.zeros((t_max-tau_max, tau_max))\n",
    "    prob_from_initial = np.zeros((t_max-tau_max, r_max))\n",
    "    for t in range(t_max-tau_max):\n",
    "        if t == 0:\n",
    "            prob_from_initial[t, 0] = 1\n",
    "        else:\n",
    "            prob_from_initial[t, 0] = np.exp(-lambda_mle_list[t-1])\n",
    "            for r in range(r_max-1):\n",
    "                prob_from_initial[t, r+1] = prob_from_initial[t, r]*lambda_mle_list[t-1]/(r+1)\n",
    "\n",
    "    for t in range(t_max-tau_max):\n",
    "        prob_from_t = np.zeros((tau_max, r_max))\n",
    "        for tau in range(tau_max): # 注意应当设置tau>=1，所以后续使用tau计算时，应考虑tau=tau+1的问题\n",
    "            if t == 0:\n",
    "                lam = lambda_mle_list[t+tau]\n",
    "            else:\n",
    "                lam = lambda_mle_list[t+tau] - lambda_mle_list[t-1]\n",
    "            prob_from_t[tau, 0] = np.exp(-lam)\n",
    "            for r in range(r_max-1):\n",
    "                prob_from_t[tau, r+1] = prob_from_t[tau, r]*lam/(r+1)\n",
    "            for i in range(r_max):\n",
    "                for j in range(i+1, r_max):\n",
    "                    if (prob_from_t[tau, j-i]*prob_from_initial[t, i]) > 0:\n",
    "                        HKS_matrix[t][tau] += prob_from_initial[t, i]*prob_from_t[tau, j-i]*prob_from_initial[t, i]*np.log(prob_from_t[tau, j-i]*prob_from_initial[t, i])\n",
    "            HKS_matrix[t][tau] = -1*HKS_matrix[t][tau]/(tau+1)\n",
    "        # print('HKS matrix ['+str(t+1)+'] done. (end at '+str(t_max-tau_max)+ ')') # 这句话用来提示进度，有点吵可以注释掉\n",
    "    \n",
    "    return HKS_matrix\n",
    "\n",
    "def add_noise(x, intens=1e-10):\n",
    "    # small noise to break degeneracy, see doc.\n",
    "    return x + intens * np.random.random_sample(x.shape)\n",
    "\n",
    "def query_neighbors(tree, x, k):\n",
    "    return tree.query(x, k=k + 1)[0][:, k]\n",
    "\n",
    "def count_neighbors(tree, x, r):\n",
    "    return tree.query_radius(x, r, count_only=True)\n",
    "\n",
    "def avgdigamma(points, dvec):\n",
    "    # This part finds number of neighbors in some radius in the marginal space\n",
    "    # returns expectation value of <psi(nx)>\n",
    "    tree = build_tree(points)\n",
    "    dvec = dvec - 1e-15\n",
    "    num_points = count_neighbors(tree, points, dvec)\n",
    "    return np.mean(digamma(num_points))\n",
    "\n",
    "def build_tree(points):\n",
    "    if points.shape[1] >= 20:\n",
    "        return BallTree(points, metric=\"chebyshev\")\n",
    "    return KDTree(points, metric=\"chebyshev\")\n",
    "\n",
    "def lnc_correction(tree, points, k, alpha):\n",
    "    e = 0\n",
    "    n_sample = points.shape[0]\n",
    "    for point in points:\n",
    "        # Find k-nearest neighbors in joint space, p=inf means max norm\n",
    "        knn = tree.query(point[None, :], k=k + 1, return_distance=False)[0]\n",
    "        knn_points = points[knn]\n",
    "        # Substract mean of k-nearest neighbor points\n",
    "        knn_points = knn_points - knn_points[0]\n",
    "        # Calculate covariance matrix of k-nearest neighbor points, obtain eigen vectors\n",
    "        covr = knn_points.T @ knn_points / k\n",
    "        _, v = la.eig(covr)\n",
    "        # Calculate PCA-bounding box using eigen vectors\n",
    "        V_rect = np.log(np.abs(knn_points @ v).max(axis=0)).sum()\n",
    "        # Calculate the volume of original box\n",
    "        log_knn_dist = np.log(np.abs(knn_points).max(axis=0)).sum()\n",
    "\n",
    "        # Perform local non-uniformity checking and update correction term\n",
    "        if V_rect < log_knn_dist + np.log(alpha):\n",
    "            e += (log_knn_dist - V_rect) / n_sample\n",
    "    return e\n",
    "\n",
    "# 用于估计序列的Shannon Entropy\n",
    "def entropy(x, k=3, base=2):\n",
    "    \"\"\"The classic K-L k-nearest neighbor continuous entropy estimator\n",
    "    x should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "    if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x = np.asarray(x)\n",
    "    n_elements, n_features = x.shape\n",
    "    x = add_noise(x)\n",
    "    tree = build_tree(x)\n",
    "    nn = query_neighbors(tree, x, k)\n",
    "    const = digamma(n_elements) - digamma(k) + n_features * np.log(2)\n",
    "    return (const + n_features * np.log(nn).mean()) / np.log(base)\n",
    "\n",
    "# 基于knn的熵估计，可以手动调一下k的数值\n",
    "def knn_entropy(data, k=3):\n",
    "    \"\"\"计算基于 k-nearest neighbor 的熵\"\"\"\n",
    "    n = len(data)\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(data.reshape(-1, 1))\n",
    "    distances, _ = neigh.kneighbors(data.reshape(-1, 1))\n",
    "    entropy = np.log(n) - np.mean(np.log(distances[:, k-1] + 1e-10))\n",
    "    return entropy\n",
    "\n",
    "# 用于估计两个序列之间的MI\n",
    "def MutualInfoEstimation(x, y, z=None, k=3, base=np.e, alpha=0.001):\n",
    "    \"\"\"Mutual information of x and y (conditioned on z if z is not None)\n",
    "    x, y should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "    if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Arrays should have same length\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x, y = np.asarray(x), np.asarray(y)\n",
    "    x, y = x.reshape(x.shape[0], -1), y.reshape(y.shape[0], -1)\n",
    "    x = add_noise(x)\n",
    "    y = add_noise(y)\n",
    "    points = [x, y]\n",
    "    if z is not None:\n",
    "        z = np.asarray(z)\n",
    "        z = z.reshape(z.shape[0], -1)\n",
    "        points.append(z)\n",
    "    points = np.hstack(points)\n",
    "    # Find nearest neighbors in joint space, p=inf means max-norm\n",
    "    tree = build_tree(points)\n",
    "    dvec = query_neighbors(tree, points, k)\n",
    "    if z is None:\n",
    "        a, b, c, d = (\n",
    "            avgdigamma(x, dvec),\n",
    "            avgdigamma(y, dvec),\n",
    "            digamma(k),\n",
    "            digamma(len(x)),\n",
    "        )\n",
    "        if alpha > 0:\n",
    "            d += lnc_correction(tree, points, k, alpha)\n",
    "    else:\n",
    "        xz = np.c_[x, z]\n",
    "        yz = np.c_[y, z]\n",
    "        a, b, c, d = (\n",
    "            avgdigamma(xz, dvec),\n",
    "            avgdigamma(yz, dvec),\n",
    "            avgdigamma(z, dvec),\n",
    "            digamma(k),\n",
    "        )\n",
    "    return (-a - b + c + d) / np.log(base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 'block.ln1.l1': Shannon Entropy average = 29.331191376137557\n"
     ]
    }
   ],
   "source": [
    "# 基于knn算法的Shannon Entropy计算demo(信息论分析)，选取了一层数据来演示\n",
    "# 基于knn算法进行的计算很有可能会不准确，可以尝试一下调整k的数值，这部分建议先跑上保存，看数据情况取舍\n",
    "\n",
    "layer_name_list = [\n",
    "    'att.key.l0','att.key.l1','att.key.l2','att.key.l3','att.key.l4','att.key.l5',\n",
    "    'att.key.l6','att.key.l7','att.key.l8','att.key.l9','att.key.l10','att.key.l11',\n",
    "    'att.value.l0','att.value.l1','att.value.l2','att.value.l3','att.value.l4','att.value.l5',\n",
    "    'att.value.l6','att.value.l7','att.value.l8','att.value.l9','att.value.l10','att.value.l11',\n",
    "    'att.receptance.l0','att.receptance.l1','att.receptance.l2','att.receptance.l3','att.receptance.l4','att.receptance.l5',\n",
    "    'att.receptance.l6','att.receptance.l7','att.receptance.l8','att.receptance.l9','att.receptance.l10','att.receptance.l11',\n",
    "    'att.sigmoid.l0','att.sigmoid.l1','att.sigmoid.l2','att.sigmoid.l3','att.sigmoid.l4','att.sigmoid.l5',\n",
    "    'att.sigmoid.l6','att.sigmoid.l7','att.sigmoid.l8','att.sigmoid.l9','att.sigmoid.l10','att.sigmoid.l11',\n",
    "    \n",
    "    'ffn.key.l0','ffn.key.l1','ffn.key.l2','ffn.key.l3','ffn.key.l4','ffn.key.l5',\n",
    "    'ffn.key.l6','ffn.key.l7','ffn.key.l8','ffn.key.l9','ffn.key.l10','ffn.key.l11',\n",
    "    'ffn.value.l0','ffn.value.l1','ffn.value.l2','ffn.value.l3','ffn.value.l4','ffn.value.l5',\n",
    "    'ffn.value.l6','ffn.value.l7','ffn.value.l8','ffn.value.l9','ffn.value.l10','ffn.value.l11',\n",
    "    'ffn.receptance.l0','ffn.receptance.l1','ffn.receptance.l2','ffn.receptance.l3','ffn.receptance.l4','ffn.receptance.l5',\n",
    "    'ffn.receptance.l6','ffn.receptance.l7','ffn.receptance.l8','ffn.receptance.l9','ffn.receptance.l10','ffn.receptance.l11',\n",
    "    'ffn.sigmoid.l0','ffn.sigmoid.l1','ffn.sigmoid.l2','ffn.sigmoid.l3','ffn.sigmoid.l4','ffn.sigmoid.l5',\n",
    "    'ffn.sigmoid.l6','ffn.sigmoid.l7','ffn.sigmoid.l8','ffn.sigmoid.l9','ffn.sigmoid.l10','ffn.sigmoid.l11',\n",
    "    \n",
    "    'block.ln1.l0','block.ln1.l1','block.ln1.l2','block.ln1.l3','block.ln1.l4','block.ln1.l5',\n",
    "    'block.ln1.l6','block.ln1.l7','block.ln1.l8','block.ln1.l9','block.ln1.l10','block.ln1.l11',\n",
    "    'block.ln2.l0','block.ln2.l1','block.ln2.l2','block.ln2.l3','block.ln2.l4','block.ln2.l5',\n",
    "    'block.ln2.l6','block.ln2.l7','block.ln2.l8','block.ln2.l9','block.ln2.l10','block.ln2.l11',\n",
    "\n",
    "    'quant','block.ln0','head'\n",
    "]\n",
    "\n",
    "# 这下面在生成文件路径读取文件\n",
    "task_name = 'arc_easy_false'\n",
    "# 如果task_name in ['winogrande_true','winogrande_false'] 则 word_id in['0','1']（winogrande数据的输出生成2个单词,最后一个单词为休止符）\n",
    "# 如果task_name in ['arc_easy_true','arc_easy_false'] 则 word_id in['0','1','2','3']（arc_easy数据的输出生成4个单词,最后一个单词为休止符）\n",
    "word_id = '2'\n",
    "\n",
    "layer_name_1 = 'block.ln1.l1'\n",
    "paths_1 = glob.glob(os.path.join(f\"/nfs/xuhan/xyh/data/data_06_04/int4/{task_name}\", f'*doc_id_*_{word_id}.{layer_name_1}.0.pth')) # 这里我放在xyh文件夹下运行，因此使用完整路径\n",
    "# t = 句子*上下文数量*神经元\n",
    "t_1 = np.squeeze(np.array([torch.load(path) for path in paths_1]))\n",
    "t_1 = t_1.reshape(-1, t_1.shape[2])\n",
    "# 脉冲数量序列转脉冲序列 (int to binary)，在现在的版本暂时弃用\n",
    "# layer_spike_1 = np.apply_along_axis(int_to_binary_random, 0, t_1)\n",
    "\n",
    "# 对该layer中的每一个neuron都进行Shannon Entropy计算\n",
    "entropy_shape = np.shape(t_1)[1] # 完整维度为layer1 neuron个数(768)，这个完整跑不怎么花时间\n",
    "entropy_array = np.zeros(entropy_shape)\n",
    "\n",
    "for neuron_1 in range(entropy_shape):\n",
    "    shannon_entropy = knn_entropy(t_1[:,neuron_1].reshape(-1,1)) # 可以手动调一下knn里k的数值\n",
    "    entropy_array[neuron_1] = shannon_entropy\n",
    "\n",
    "# 输出结果\n",
    "print('layer \\''+layer_name_1+'\\': Shannon Entropy average = '+str(np.mean(entropy_array)))\n",
    "# np.save('/nfs/xuhan/xyh/results/entropy/'+(layer_name_1)+'.npy', entropy_array) # 需要根据task_name和word_id调整保存路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'arc_easy_false'\n",
    "# 如果task_name in ['winogrande_true','winogrande_false'] 则 word_id in['0','1']（winogrande数据的输出生成2个单词,最后一个单词为休止符）\n",
    "# 如果task_name in ['arc_easy_true','arc_easy_false'] 则 word_id in['0','1','2','3']（arc_easy数据的输出生成4个单词,最后一个单词为休止符）\n",
    "word_id = '3'\n",
    "\n",
    "layer_name_1 = 'block.ln1.l1'\n",
    "\n",
    "paths_1 = glob.glob(os.path.join(f\"/nfs/xuhan/xyh/data/data_06_04/int4/{task_name}\", f'*doc_id_*_{word_id}.{layer_name_1}.0.pth'))\n",
    "data_list_1 = [torch.load(path) for path in paths_1]\n",
    "adjusted_data_list_1 = []\n",
    "for data in data_list_1:\n",
    "    if len(data.shape) == 3 and data.shape[0] == 1:  # 如果多了一个维度，形状为(1, 30, 768)\n",
    "        adjusted_data_list_1.append(data.squeeze(0))  # 去掉第一个维度\n",
    "    else:\n",
    "        adjusted_data_list_1.append(data)\n",
    "    \n",
    "t_1 = np.concatenate([data.numpy().reshape(-1, data.shape[-1]) for data in adjusted_data_list_1], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 'ffn.receptance.l4': MI average = 0.3153509012373951\n"
     ]
    }
   ],
   "source": [
    "# 互信息计算demo (信息论分析)，使用了两层数据来演示\n",
    "# 直接使用了mi估计器进行运算，在非独立二维高斯分布下的实验结果与理论值对应较好，作为正式mi计算的工具\n",
    "\n",
    "layer_name_list = [\n",
    "    'att.key.l0','att.key.l1','att.key.l2','att.key.l3','att.key.l4','att.key.l5',\n",
    "    'att.key.l6','att.key.l7','att.key.l8','att.key.l9','att.key.l10','att.key.l11',\n",
    "    'att.value.l0','att.value.l1','att.value.l2','att.value.l3','att.value.l4','att.value.l5',\n",
    "    'att.value.l6','att.value.l7','att.value.l8','att.value.l9','att.value.l10','att.value.l11',\n",
    "    'att.receptance.l0','att.receptance.l1','att.receptance.l2','att.receptance.l3','att.receptance.l4','att.receptance.l5',\n",
    "    'att.receptance.l6','att.receptance.l7','att.receptance.l8','att.receptance.l9','att.receptance.l10','att.receptance.l11',\n",
    "    'att.sigmoid.l0','att.sigmoid.l1','att.sigmoid.l2','att.sigmoid.l3','att.sigmoid.l4','att.sigmoid.l5',\n",
    "    'att.sigmoid.l6','att.sigmoid.l7','att.sigmoid.l8','att.sigmoid.l9','att.sigmoid.l10','att.sigmoid.l11',\n",
    "    \n",
    "    'ffn.key.l0','ffn.key.l1','ffn.key.l2','ffn.key.l3','ffn.key.l4','ffn.key.l5',\n",
    "    'ffn.key.l6','ffn.key.l7','ffn.key.l8','ffn.key.l9','ffn.key.l10','ffn.key.l11',\n",
    "    'ffn.value.l0','ffn.value.l1','ffn.value.l2','ffn.value.l3','ffn.value.l4','ffn.value.l5',\n",
    "    'ffn.value.l6','ffn.value.l7','ffn.value.l8','ffn.value.l9','ffn.value.l10','ffn.value.l11',\n",
    "    'ffn.receptance.l0','ffn.receptance.l1','ffn.receptance.l2','ffn.receptance.l3','ffn.receptance.l4','ffn.receptance.l5',\n",
    "    'ffn.receptance.l6','ffn.receptance.l7','ffn.receptance.l8','ffn.receptance.l9','ffn.receptance.l10','ffn.receptance.l11',\n",
    "    'ffn.sigmoid.l0','ffn.sigmoid.l1','ffn.sigmoid.l2','ffn.sigmoid.l3','ffn.sigmoid.l4','ffn.sigmoid.l5',\n",
    "    'ffn.sigmoid.l6','ffn.sigmoid.l7','ffn.sigmoid.l8','ffn.sigmoid.l9','ffn.sigmoid.l10','ffn.sigmoid.l11',\n",
    "    \n",
    "    'block.ln1.l0','block.ln1.l1','block.ln1.l2','block.ln1.l3','block.ln1.l4','block.ln1.l5',\n",
    "    'block.ln1.l6','block.ln1.l7','block.ln1.l8','block.ln1.l9','block.ln1.l10','block.ln1.l11',\n",
    "    'block.ln2.l0','block.ln2.l1','block.ln2.l2','block.ln2.l3','block.ln2.l4','block.ln2.l5',\n",
    "    'block.ln2.l6','block.ln2.l7','block.ln2.l8','block.ln2.l9','block.ln2.l10','block.ln2.l11',\n",
    "\n",
    "    'quant','block.ln0','head'\n",
    "]\n",
    "\n",
    "# 这下面在生成文件路径读取文件\n",
    "task_name = 'arc_easy_true'\n",
    "# 如果task_name in ['winogrande_true','winogrande_false'] 则 word_id in['0','1']（winogrande数据的输出生成2个单词,最后一个单词为休止符）\n",
    "# 如果task_name in ['arc_easy_true','arc_easy_false'] 则 word_id in['0','1','2','3']（arc_easy数据的输出生成4个单词,最后一个单词为休止符）\n",
    "word_id = '0'\n",
    "\n",
    "layer_name_1 = 'quant' # 这里替换成初始输入层的layer_name\n",
    "paths_1 = glob.glob(os.path.join(f\"/nfs/xuhan/xyh/data/data_06_04/int4/{task_name}\", f'*doc_id_*_{word_id}.{layer_name_1}.0.pth')) # 这里我放在xyh文件夹下运行，因此使用完整路径\n",
    "# t = 句子*上下文数量*神经元\n",
    "t_1 = np.squeeze(np.array([torch.load(path) for path in paths_1]))\n",
    "t_1 = t_1.reshape(-1, t_1.shape[2])\n",
    "# 脉冲数量序列转脉冲序列 (int to binary)，在这个版本暂时不用\n",
    "# layer_spike_1 = np.apply_along_axis(int_to_binary_random, 0, t_1)\n",
    "\n",
    "layer_name_2 = 'ffn.receptance.l4'\n",
    "# for layer_name_2 in layer_name_list:\n",
    "paths_2 = glob.glob(os.path.join(f\"/nfs/xuhan/xyh/data/data_06_04/int4/{task_name}\", f'*doc_id_*_{word_id}.{layer_name_2}.0.pth')) # 这里我放在xyh文件夹下运行，因此使用完整路径\n",
    "t_2 = np.squeeze(np.array([torch.load(path) for path in paths_2]))\n",
    "t_2 = t_2.reshape(-1, t_2.shape[2])\n",
    "# 脉冲数量序列转脉冲序列 (int to binary)，在这个版本暂时不用\n",
    "# layer_spike_2 = np.apply_along_axis(int_to_binary_random, 0, t_2)\n",
    "\n",
    "# 完整跑所有pair的mi的代码\n",
    "# mi_shape = (np.shape(t_1)[1], np.shape(t_2)[1]) # 完整维度为layer1 neuron个数(768)*layer2 neuron个数(768)，建议随机抽样跑(2% = 10000)，完整跑一遍所有pair的mi太花时间了orz，对于两层的数据大概完整跑一遍要花10h\n",
    "# mi_matrix = np.zeros(mi_shape)\n",
    "# for neuron_1 in range(mi_shape[0]):\n",
    "#     for neuron_2 in range(mi_shape[1]):\n",
    "#         mi = MutualInfoEstimation(t_1[:,neuron_1].reshape(-1,1), t_2[:,neuron_2].reshape(-1,1))\n",
    "#         mi_matrix[neuron_1][neuron_2] = mi\n",
    "#     # print(str(layer_name_2)+' neuron: '+str(neuron_1+1)+'/'+str(mi_shape[0])+' done.')\n",
    "    \n",
    "# 随机抽样跑mi的代码，10000pair大概跑10min\n",
    "mi_shape = 10000 # 抽样数10000 pair\n",
    "mi_matrix = np.zeros(mi_shape)\n",
    "for pair in range(mi_shape):\n",
    "    mi = MutualInfoEstimation(t_1[:,np.random.randint(np.shape(t_1)[1])].reshape(-1,1), t_2[:,np.random.randint(np.shape(t_2)[1])].reshape(-1,1))\n",
    "    mi_matrix[pair] = mi\n",
    "\n",
    "# 输出结果\n",
    "print('layer \\''+layer_name_2+'\\': MI average = '+str(np.mean(mi_matrix)))\n",
    "np.save('/nfs/xuhan/xyh/results/mi/'+(layer_name_2)+'.npy', mi_matrix) # 需要根据task_name和word_id调整保存路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy_1 average = 29.333093314682355\n",
      "entropy_2 average = 29.337897446085975\n",
      "entropy_joint average = 29.387027479848403\n",
      "MI average = 0.23237186116855227\n",
      "MI (joint) average = 29.28396328091993\n"
     ]
    }
   ],
   "source": [
    "# 互信息计算demo (信息论分析)\n",
    "# 这个例子说明了knn算法的问题，请不要正式运行这个代码块的文件来计算mi！\n",
    "# 不要运行这段！\n",
    "\n",
    "layer_name_list = [\n",
    "    'att.key.l0','att.key.l1','att.key.l2','att.key.l3','att.key.l4','att.key.l5',\n",
    "    'att.key.l6','att.key.l7','att.key.l8','att.key.l9','att.key.l10','att.key.l11',\n",
    "    'att.value.l0','att.value.l1','att.value.l2','att.value.l3','att.value.l4','att.value.l5',\n",
    "    'att.value.l6','att.value.l7','att.value.l8','att.value.l9','att.value.l10','att.value.l11',\n",
    "    'att.receptance.l0','att.receptance.l1','att.receptance.l2','att.receptance.l3','att.receptance.l4','att.receptance.l5',\n",
    "    'att.receptance.l6','att.receptance.l7','att.receptance.l8','att.receptance.l9','att.receptance.l10','att.receptance.l11',\n",
    "    'att.sigmoid.l0','att.sigmoid.l1','att.sigmoid.l2','att.sigmoid.l3','att.sigmoid.l4','att.sigmoid.l5',\n",
    "    'att.sigmoid.l6','att.sigmoid.l7','att.sigmoid.l8','att.sigmoid.l9','att.sigmoid.l10','att.sigmoid.l11',\n",
    "    \n",
    "    'ffn.key.l0','ffn.key.l1','ffn.key.l2','ffn.key.l3','ffn.key.l4','ffn.key.l5',\n",
    "    'ffn.key.l6','ffn.key.l7','ffn.key.l8','ffn.key.l9','ffn.key.l10','ffn.key.l11',\n",
    "    'ffn.value.l0','ffn.value.l1','ffn.value.l2','ffn.value.l3','ffn.value.l4','ffn.value.l5',\n",
    "    'ffn.value.l6','ffn.value.l7','ffn.value.l8','ffn.value.l9','ffn.value.l10','ffn.value.l11',\n",
    "    'ffn.receptance.l0','ffn.receptance.l1','ffn.receptance.l2','ffn.receptance.l3','ffn.receptance.l4','ffn.receptance.l5',\n",
    "    'ffn.receptance.l6','ffn.receptance.l7','ffn.receptance.l8','ffn.receptance.l9','ffn.receptance.l10','ffn.receptance.l11',\n",
    "    'ffn.sigmoid.l0','ffn.sigmoid.l1','ffn.sigmoid.l2','ffn.sigmoid.l3','ffn.sigmoid.l4','ffn.sigmoid.l5',\n",
    "    'ffn.sigmoid.l6','ffn.sigmoid.l7','ffn.sigmoid.l8','ffn.sigmoid.l9','ffn.sigmoid.l10','ffn.sigmoid.l11',\n",
    "    \n",
    "    'block.ln1.l0','block.ln1.l1','block.ln1.l2','block.ln1.l3','block.ln1.l4','block.ln1.l5',\n",
    "    'block.ln1.l6','block.ln1.l7','block.ln1.l8','block.ln1.l9','block.ln1.l10','block.ln1.l11',\n",
    "    'block.ln2.l0','block.ln2.l1','block.ln2.l2','block.ln2.l3','block.ln2.l4','block.ln2.l5',\n",
    "    'block.ln2.l6','block.ln2.l7','block.ln2.l8','block.ln2.l9','block.ln2.l10','block.ln2.l11',\n",
    "\n",
    "    'quant','block.ln0','head'\n",
    "]\n",
    "\n",
    "task_name = 'arc_easy_true'\n",
    "# 如果task_name in ['winogrande_true','winogrande_false'] 则 word_id in['0','1']（winogrande数据的输出生成2个单词,最后一个单词为休止符）\n",
    "# 如果task_name in ['arc_easy_true','arc_easy_false'] 则 word_id in['0','1','2','3']（arc_easy数据的输出生成4个单词,最后一个单词为休止符）\n",
    "word_id = '0'\n",
    "\n",
    "layer_name_1 = 'quant'\n",
    "paths_1 = glob.glob(os.path.join(f\"/nfs/xuhan/xyh/data/data_06_04/int4/{task_name}\", f'*doc_id_*_{word_id}.{layer_name_1}.0.pth')) # 这里我放在xyh文件夹下运行，因此使用完整路径\n",
    "# t = 句子*上下文数量*神经元\n",
    "t_1 = np.squeeze(np.array([torch.load(path) for path in paths_1]))\n",
    "t_1 = t_1.reshape(-1, t_1.shape[2])\n",
    "# 脉冲数量序列转脉冲序列 (int to binary)\n",
    "# layer_spike_1 = np.apply_along_axis(int_to_binary_random, 0, t_1)\n",
    "\n",
    "layer_name_2 = 'att.receptance.l7'\n",
    "paths_2 = glob.glob(os.path.join(f\"/nfs/xuhan/xyh/data/data_06_04/int4/{task_name}\", f'*doc_id_*_{word_id}.{layer_name_2}.0.pth')) # 这里我放在xyh文件夹下运行，因此使用完整路径\n",
    "t_2 = np.squeeze(np.array([torch.load(path) for path in paths_2]))\n",
    "t_2 = t_2.reshape(-1, t_2.shape[2])\n",
    "# 脉冲数量序列转脉冲序列 (int to binary)\n",
    "# layer_spike_2 = np.apply_along_axis(int_to_binary_random, 0, t_2)\n",
    "\n",
    "# mi_shape = (np.shape(t_1)[1], np.shape(t_2)[1])\n",
    "mi_shape = (40, 40) # 取40*40个neuron简单算算\n",
    "mi_matrix = np.zeros(mi_shape)\n",
    "mi_matrix_t = np.zeros(mi_shape)\n",
    "joint_entropy_matrix = np.zeros(mi_shape)\n",
    "entropy_1_list = np.zeros(mi_shape[0])\n",
    "entropy_2_list = np.zeros(mi_shape[1])\n",
    "\n",
    "for neuron_1 in range(mi_shape[0]):\n",
    "    entropy_1_list[neuron_1] = knn_entropy(t_1[:,neuron_1].reshape(-1,1))\n",
    "for neuron_2 in range(mi_shape[1]):\n",
    "    entropy_2_list[neuron_2] = knn_entropy(t_2[:,neuron_2].reshape(-1,1))\n",
    "\n",
    "# 直接用估计器计算mi\n",
    "for neuron_1 in range(mi_shape[0]):\n",
    "    for neuron_2 in range(mi_shape[1]):\n",
    "        mi = MutualInfoEstimation(t_1[:,neuron_1].reshape(-1,1), t_2[:,neuron_2].reshape(-1,1))\n",
    "        mutual_info_score\n",
    "        # mi = (mi>=0)*mi\n",
    "        mi_matrix[neuron_1][neuron_2] = mi\n",
    "    # print('neuron: '+str(neuron_1+1)+'/'+str(mi_shape[0])+' done.')\n",
    "\n",
    "# 通过联合分布熵计算mi，熵估计使用knn\n",
    "for neuron_1 in range(mi_shape[0]):\n",
    "    for neuron_2 in range(mi_shape[1]):\n",
    "        joint = np.hstack((t_1[:,neuron_1].reshape(-1,1), t_2[:,neuron_2].reshape(-1,1)))\n",
    "        joint_entropy_matrix[neuron_1][neuron_2] = knn_entropy(joint)\n",
    "    # print('neuron (joint): '+str(neuron_1+1)+'/'+str(mi_shape[0])+' done.')\n",
    "\n",
    "# 联合熵算出的惊人的mi\n",
    "print('entropy_1 average = '+str(np.mean(entropy_1_list)))\n",
    "print('entropy_2 average = '+str(np.mean(entropy_2_list)))\n",
    "print('entropy_joint average = '+str(np.mean(joint_entropy_matrix)))\n",
    "print('MI average = '+str(np.mean(mi_matrix))) # 直接使用mi估计器计算得到的mi\n",
    "print('MI (joint) average = '+str(np.mean(entropy_1_list)+np.mean(entropy_2_list)-np.mean(joint_entropy_matrix))) # 用knn进行信息熵估计，再通过边缘分布和联合分布概率熵的关系计算mi=entropy_1+entropy_2-entropy_joint\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
